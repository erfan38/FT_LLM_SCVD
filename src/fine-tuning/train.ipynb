{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955cec52-2058-459d-b004-70e6bc391c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: datasets in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (3.3.1)\n",
      "Collecting peft\n",
      "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-win_amd64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: psutil in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from peft) (2.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: networkx in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\erfan\\onedrive - polymtlus\\apps\\envs\\myenv\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.6/10.4 MB 7.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.9/10.4 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.7/10.4 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.3/10.4 MB 7.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.4/10.4 MB 7.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.0/10.4 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.4/10.4 MB 7.7 MB/s eta 0:00:00\n",
      "Downloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Downloading bitsandbytes-0.45.5-py3-none-win_amd64.whl (75.4 MB)\n",
      "   ---------------------------------------- 0.0/75.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.6/75.4 MB 9.3 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 3.4/75.4 MB 8.7 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 5.2/75.4 MB 8.6 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 6.8/75.4 MB 8.4 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 8.4/75.4 MB 8.1 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 10.2/75.4 MB 8.2 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 12.1/75.4 MB 8.1 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 13.9/75.4 MB 8.2 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 15.5/75.4 MB 8.2 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 17.3/75.4 MB 8.3 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 19.4/75.4 MB 8.3 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 21.2/75.4 MB 8.3 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 22.8/75.4 MB 8.3 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 24.6/75.4 MB 8.3 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 26.5/75.4 MB 8.3 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 28.6/75.4 MB 8.4 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 30.4/75.4 MB 8.4 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 32.2/75.4 MB 8.4 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 34.1/75.4 MB 8.4 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 35.9/75.4 MB 8.5 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 38.0/75.4 MB 8.5 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 39.8/75.4 MB 8.5 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 41.7/75.4 MB 8.5 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 43.5/75.4 MB 8.5 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 45.4/75.4 MB 8.5 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 47.2/75.4 MB 8.6 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 49.3/75.4 MB 8.5 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 51.1/75.4 MB 8.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 53.2/75.4 MB 8.6 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 55.3/75.4 MB 8.6 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 57.1/75.4 MB 8.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 59.0/75.4 MB 8.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 60.8/75.4 MB 8.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 62.9/75.4 MB 8.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 64.7/75.4 MB 8.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 66.6/75.4 MB 8.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 68.4/75.4 MB 8.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 70.3/75.4 MB 8.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 72.4/75.4 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  74.2/75.4 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  75.2/75.4 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 75.4/75.4 MB 8.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 1.8/2.4 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 8.2 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, bitsandbytes, accelerate, transformers, peft\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.29.0\n",
      "    Uninstalling huggingface-hub-0.29.0:\n",
      "      Successfully uninstalled huggingface-hub-0.29.0\n",
      "Successfully installed accelerate-1.6.0 bitsandbytes-0.45.5 huggingface-hub-0.30.2 peft-0.15.2 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers datasets peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e05ef80-a70e-4995-a825-1f216d845ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from datasets import load_dataset\n",
    "# from transformers import (\n",
    "#     AutoTokenizer,\n",
    "#     AutoModelForCausalLM,\n",
    "#     Trainer,\n",
    "#     TrainingArguments,\n",
    "#     DataCollatorForSeq2Seq,\n",
    "# )\n",
    "# from peft import (\n",
    "#     LoraConfig,\n",
    "#     SoraConfig,\n",
    "#     get_peft_model,\n",
    "#     prepare_model_for_kbit_training,\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adebed79-8936-4458-875b-897143785277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configuration\n",
    "# model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Replace with your chosen model\n",
    "# use_sora = False  # Set to True for SoRA, False for LoRA\n",
    "\n",
    "# # Load dataset\n",
    "# dataset = load_dataset(\"json\", data_files=\"train-split.jsonl\")[\"train\"]\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "# )\n",
    "# # Preprocess function\n",
    "# def preprocess(example):\n",
    "#     messages = example[\"messages\"]\n",
    "#     prompt = \"\"\n",
    "#     for msg in messages:\n",
    "#         role = msg[\"role\"]\n",
    "#         content = msg[\"content\"]\n",
    "#         if role == \"system\":\n",
    "#             prompt += f\"<s>[INST] {content} [/INST]\"\n",
    "#         elif role == \"user\":\n",
    "#             prompt += f\" {content} \"\n",
    "#         elif role == \"assistant\":\n",
    "#             prompt += f\"{content}</s>\"\n",
    "#     return tokenizer(prompt, truncation=True, max_length=512)\n",
    "\n",
    "# # Tokenize dataset\n",
    "# tokenized_dataset = dataset.map(preprocess)\n",
    "\n",
    "# # Configure PEFT\n",
    "# if use_sora:\n",
    "#     peft_config = SoraConfig(\n",
    "#         r=8,\n",
    "#         lora_alpha=16,\n",
    "#         lora_dropout=0.05,\n",
    "#         bias=\"none\",\n",
    "#         task_type=\"CAUSAL_LM\",\n",
    "#     )\n",
    "# else:\n",
    "#     peft_config = LoraConfig(\n",
    "#         r=8,\n",
    "#         lora_alpha=16,\n",
    "#         lora_dropout=0.05,\n",
    "#         bias=\"none\",\n",
    "#         task_type=\"CAUSAL_LM\",\n",
    "#     )\n",
    "\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# model = get_peft_model(model, peft_config)\n",
    "\n",
    "# # Training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results-sora\" if use_sora else \"./results-lora\",\n",
    "#     per_device_train_batch_size=2,\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     learning_rate=2e-4,\n",
    "#     logging_steps=10,\n",
    "#     num_train_epochs=3,\n",
    "#     evaluation_strategy=\"no\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     fp16=True,\n",
    "#     save_total_limit=2,\n",
    "# )\n",
    "\n",
    "# # Initialize Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=DataCollatorForSeq2Seq(tokenizer, padding=True),\n",
    "# )\n",
    "\n",
    "# # Start training\n",
    "# trainer.train()\n",
    "\n",
    "# # Save the final model\n",
    "# output_dir = \"./final-model-sora\" if use_sora else \"./final-model-lora\"\n",
    "# model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b40ca4b-b1d3-40cd-92f3-ae2898d0fcda",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_split.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_split.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f_in, \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msora_data.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_out:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f_in:\n\u001b[0;32m      5\u001b[0m         item \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n",
      "File \u001b[1;32m~\\OneDrive - polymtlus\\Apps\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_split.jsonl'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"train_split.jsonl\") as f_in, open(\"sora_data.jsonl\", \"w\") as f_out:\n",
    "    for line in f_in:\n",
    "        item = json.loads(line)\n",
    "        messages = item[\"messages\"]\n",
    "        prompt = \"\"\n",
    "        output = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                prompt += f\"<|system|>\\n{msg['content'].strip()}\\n\"\n",
    "            elif msg[\"role\"] == \"user\":\n",
    "                prompt += f\"<|user|>\\n{msg['content'].strip()}\\n\"\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                output += msg[\"content\"].strip() + \"\\n\"\n",
    "        final = {\n",
    "            \"input\": prompt + \"<|assistant|>\\n\",\n",
    "            \"output\": output.strip()\n",
    "        }\n",
    "        f_out.write(json.dumps(final) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dcfbe3-21c3-4d02-8512-36bb7ae917cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
