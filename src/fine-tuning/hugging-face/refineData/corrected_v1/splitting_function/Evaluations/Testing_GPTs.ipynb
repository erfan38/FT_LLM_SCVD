{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee0f8fd-5c5b-4170-8797-896068559648",
   "metadata": {},
   "source": [
    "## API calling for prediction - Saving results as JSON structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd32a99-469c-417b-8bba-2128880465c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = \"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\"\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "input_file = \"testing/prompt.json\"\n",
    "output_file = \"testing/predict_4_1mini_structed.json\"\n",
    "\n",
    "json_template = '''{\n",
    "  \"vulnerableLines\": \"\",\n",
    "  \"vulnerableCode\": [],\n",
    "  \"vulnerabilityReason\": \"\",\n",
    "  \"potentialSecurityRisk\": \"\",\n",
    "  \"fixedCode\": \"\"\n",
    "}\n",
    "'''\n",
    "\n",
    "instruction = (\n",
    "    \"You are a careful cyber-security expert who analyzes smart contracts. \"\n",
    "    \"Detect all vulnerabilities in the code and provide your answer ONLY in the following strict JSON format. \"\n",
    "    \"Return ONLY valid RFC8259-compliant JSON, with no explanation or commentary outside the JSON:\\n\"\n",
    "    + json_template +\n",
    "    \"\\nSmart contract:\\n\"\n",
    ")\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    prompts = json.load(f)\n",
    "\n",
    "prompts = prompts[:65]\n",
    "predictions = []\n",
    "\n",
    "for i, item in enumerate(prompts):\n",
    "    code = item[\"prompt\"]\n",
    "    full_prompt = instruction + code\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\", #gpt-4.1-mini #gpt-4o-mini\n",
    "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "            temperature=0  # deterministic, adjust if needed\n",
    "        )\n",
    "        prediction = completion.choices[0].message.content\n",
    "        predictions.append({\"predict\": prediction})\n",
    "        print(f\"Prompt {i+1} processed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Prompt {i+1} failed:\", e)\n",
    "        predictions.append({\"predict\": \"\"})\n",
    "\n",
    "# Save predictions to file\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(predictions, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Done. {len(predictions)} predictions saved to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa2a0fb3-d8c2-40ea-a7fa-d85ece2a515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26434f2d-650d-45ef-8dde-d5570d516d0a",
   "metadata": {},
   "source": [
    "## Separating prompt and label from test json file of fine-tuned llama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b55caea9-ca18-480b-b727-c90352fff0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1628 prompts and 1628 labels saved in 'testing' folder: done\n"
     ]
    }
   ],
   "source": [
    "input_file = \"generated_predictions_all_llama31_2.jsonl\"\n",
    "output_dir = \"testing\"\n",
    "prompt_file = os.path.join(output_dir, \"prompt.json\")\n",
    "label_file = os.path.join(output_dir, \"label.json\")\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Prepare containers\n",
    "prompts = []\n",
    "labels = []\n",
    "\n",
    "# Read the jsonl and extract fields\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            if \"prompt\" in data:\n",
    "                prompts.append({\"prompt\": data[\"prompt\"]})\n",
    "            if \"label\" in data:\n",
    "                labels.append({\"label\": data[\"label\"]})\n",
    "        except Exception as e:\n",
    "            print(\"Skipping line due to error:\", e)\n",
    "            continue\n",
    "\n",
    "# Save the extracted fields\n",
    "with open(prompt_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(prompts, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(label_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(labels, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"{len(prompts)} prompts and {len(labels)} labels saved in '{output_dir}' folder: done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c717e5-1c8b-4f2b-8353-83dd7213582a",
   "metadata": {},
   "source": [
    "## API Calling - without JSON Structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237edcb0-f6ee-4b3f-9e36-377c98bf15d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = \"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\"\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "input_file = \"testing/prompt.json\"\n",
    "output_file = \"testing/predict_4omini.json\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    prompts = json.load(f)\n",
    "    \n",
    "prompts = prompts[:65]\n",
    "predictions = []\n",
    "\n",
    "for i, item in enumerate(prompts):\n",
    "    prompt = item[\"prompt\"]\n",
    "    # Send the prompt to OpenAI (GPT-4o recommended)\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\", #model=gpt-4.1-mini\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0  # deterministic, adjust if needed\n",
    "        )\n",
    "        # Take only the text of the response\n",
    "        prediction = completion.choices[0].message.content\n",
    "        predictions.append({\"predict\": prediction})\n",
    "        print(f\"Prompt {i+1} processed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Prompt {i+1} failed:\", e)\n",
    "        predictions.append({\"predict\": \"\"})\n",
    "\n",
    "# Save predictions to file\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(predictions, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Done. {len(predictions)} predictions saved to '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a085c1-a03e-4930-b85c-a8c48fc68213",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"testing/predict_4omini.json\"\n",
    "with open(input_dir,\"r\", encoding=\"utf-8\") as f:\n",
    "    predict = json.load(f)\n",
    "    \n",
    "a = predict[0]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7359ed-2901-4f9d-b2a9-37fbcbc064c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6dbbce0-a54e-4b19-a16e-ba819fa6f45a",
   "metadata": {},
   "source": [
    "## Testing llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea1716-ba0b-43f2-9b63-d86adb58ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monsterapi import MonsterAPI\n",
    "print(MonsterAPI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0121dffa-319c-4d7c-bea6-3b36fbbc3684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import monsterapi\n",
    "print(monsterapi.__file__)\n",
    "print(monsterapi.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ba22ac2-19db-422c-8ff1-f3e0bb168d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dreambooth_config', 'InputDataModels', 'LLM_config', 'MClient', 'Whisper_config', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'client', 'deployDataModels', 'utils']\n"
     ]
    }
   ],
   "source": [
    "import monsterapi\n",
    "print(dir(monsterapi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fc2a47-ecfd-454d-90ec-59567f93f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall monsterapi -y\n",
    "!pip install --upgrade monsterapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9c67373-9c93-4b42-809d-88b7d92ff14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONSTER_API_KEY = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VybmFtZSI6ImEyY2EwMzRmMWE3ZTNhMjE3NmY3OGQ3MDJlOGFlZDljIiwiY3JlYXRlZF9hdCI6IjIwMjUtMDUtMjlUMTk6MjQ6MzQuMzczOTk0In0.NsI6YoA6zMRcEYT2JkgUAk0WYb-JMB0dxyvtwb-RLx4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d317d4-8381-4528-b04d-78dd1108c05e",
   "metadata": {},
   "source": [
    "## first you need to find the name of your model in this monster API :|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dfb520b-46f6-4b88-8efc-92c5daaf0e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ID: google/gemma-2-9b-it\n",
      "\n",
      "Model ID: meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "\n",
      "Model ID: monsterapi/Llama3.3_70b\n",
      "\n",
      "Model ID: deepseek-ai/DeepSeek-V3\n",
      "\n",
      "Model ID: google/gemini-2.0-flash\n",
      "\n",
      "Model ID: bobble-custom-model-v1\n",
      "\n",
      "Model ID: google/gemini-2.0-flash-lite\n",
      "\n",
      "Model ID: custom-model-v1-lite\n",
      "\n",
      "Model ID: google/gemma-3-27b-it\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# MonsterAPI LLM model listing endpoint\n",
    "url = \"https://llm.monsterapi.ai/v1/models\"\n",
    "\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "models = response.json()\n",
    "\n",
    "# Pretty print all model IDs and descriptions\n",
    "for model in models.get(\"data\", []):\n",
    "    print(f\"Model ID: {model['id']}\")\n",
    "    if \"description\" in model:\n",
    "        print(f\"  Description: {model['description']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "884df3df-de54-4518-819f-54714228aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from monsterapi.client import MClient\n",
    "\n",
    "# client = MClient(api_key=MONSTER_API_KEY)\n",
    "# print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "036260fe-0726-4b49-b04e-0a8e256d5210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1 processed.\n",
      "Prompt 2 processed.\n",
      "Prompt 3 processed.\n",
      "Prompt 4 processed.\n",
      "Prompt 5 processed.\n",
      "Prompt 6 processed.\n",
      "Prompt 7 processed.\n",
      "Prompt 8 processed.\n",
      "Prompt 9 processed.\n",
      "Prompt 10 processed.\n",
      "Prompt 11 processed.\n",
      "Prompt 12 processed.\n",
      "Prompt 13 processed.\n",
      "Prompt 14 processed.\n",
      "Prompt 15 processed.\n",
      "Prompt 16 processed.\n",
      "Prompt 17 processed.\n",
      "Prompt 18 processed.\n",
      "Prompt 19 processed.\n",
      "Prompt 20 processed.\n",
      "Prompt 21 processed.\n",
      "Prompt 22 processed.\n",
      "Prompt 23 processed.\n",
      "Prompt 24 processed.\n",
      "Prompt 25 processed.\n",
      "Prompt 26 processed.\n",
      "Prompt 27 processed.\n",
      "Prompt 28 processed.\n",
      "Prompt 29 processed.\n",
      "Prompt 30 processed.\n",
      "Prompt 31 processed.\n",
      "Prompt 32 processed.\n",
      "Prompt 33 processed.\n",
      "Prompt 34 processed.\n",
      "Prompt 35 processed.\n",
      "Prompt 36 processed.\n",
      "Prompt 37 processed.\n",
      "Prompt 38 processed.\n",
      "Prompt 39 processed.\n",
      "Prompt 40 processed.\n",
      "Prompt 41 processed.\n",
      "Prompt 42 processed.\n",
      "Prompt 43 processed.\n",
      "Prompt 44 processed.\n",
      "Prompt 45 processed.\n",
      "Prompt 46 processed.\n",
      "Prompt 47 processed.\n",
      "Prompt 48 processed.\n",
      "Prompt 49 processed.\n",
      "Prompt 50 processed.\n",
      "Prompt 51 processed.\n",
      "Prompt 52 processed.\n",
      "Prompt 53 processed.\n",
      "Prompt 54 processed.\n",
      "Prompt 55 processed.\n",
      "Prompt 56 processed.\n",
      "Prompt 57 processed.\n",
      "Prompt 58 processed.\n",
      "Prompt 59 processed.\n",
      "Prompt 60 processed.\n",
      "Prompt 61 processed.\n",
      "Prompt 62 processed.\n",
      "Prompt 63 processed.\n",
      "Prompt 64 processed.\n",
      "Prompt 65 processed.\n",
      "Done. 65 predictions saved to 'testing/predict_llama31_8b.json'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "input_file = \"testing/prompt.json\"\n",
    "output_file = \"testing/predict_raw_llama31_8b.json\"\n",
    "\n",
    "url = \"https://llm.monsterapi.ai/v1/chat/completions\"  #### finally I found the correct endpoint url :|\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"content-type\": \"application/json\",\n",
    "    \"authorization\": f\"Bearer {MONSTER_API_KEY}\" ## bearer format :|\n",
    "}\n",
    "\n",
    "json_template = '''{\n",
    "  \"vulnerableLines\": \"\",\n",
    "  \"vulnerableCode\": [],\n",
    "  \"vulnerabilityReason\": \"\",\n",
    "  \"potentialSecurityRisk\": \"\",\n",
    "  \"fixedCode\": \"\"\n",
    "}\n",
    "'''\n",
    "############################### i prepared instruction with a JSON template\n",
    "instruction = (\n",
    "    \"You are a careful cyber-security expert who analyzes smart contracts. \"\n",
    "    \"Detect all vulnerabilities in the code and provide your answer ONLY in the following strict JSON format:\\n\"\n",
    "    + json_template +\n",
    "    \"\\nSmart contract:\\n\"\n",
    ")\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    prompts = json.load(f)\n",
    "\n",
    "prompts = prompts[:65]  \n",
    "predictions = []\n",
    "\n",
    "for i, item in enumerate(prompts):\n",
    "    prompt = item[\"prompt\"]\n",
    "    full_prompt = instruction + prompt\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", ####### MODEL name :|\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": full_prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 256,\n",
    "        \"temperature\": 0,\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        result = response.json()\n",
    "        # Print the full result for debug if you want:\n",
    "        # print(json.dumps(result, indent=2))\n",
    "        if response.status_code != 200 or \"choices\" not in result:\n",
    "            print(f\"Prompt {i+1} failed: {response.text}\")\n",
    "            predictions.append({\"predict\": \"\"})\n",
    "            continue\n",
    "\n",
    "        answer = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        predictions.append({\"predict\": answer})\n",
    "        print(f\"Prompt {i+1} processed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Prompt {i+1} failed:\", e)\n",
    "        predictions.append({\"predict\": \"\"})\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(predictions, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Done. {len(predictions)} predictions saved to '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f43b04-ed9b-44cd-a940-75e784c7e36c",
   "metadata": {},
   "source": [
    "## Merging prediction and label for 4omini and 4.1mini and LLama3.1-8b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7f71fe6-05e6-4b7c-9b48-78dcea5f2482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged data to testing/complete_predict_4omini_structed.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Paths\n",
    "predict_path = 'testing/predict_4omini_structed.json' #predict_4omini_structed.json  #predict_4_1mini_structed.json\n",
    "label_path = 'testing/label.json'\n",
    "output_path = 'testing/complete_predict_4omini_structed.json'\n",
    "\n",
    "# Load predict.json (assumed to be a list of dicts or strings)\n",
    "with open(predict_path, 'r', encoding='utf-8') as f:\n",
    "    predicts = json.load(f)\n",
    "\n",
    "# Load label.json (assumed to be a list of dicts or strings)\n",
    "with open(label_path, 'r', encoding='utf-8') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "# Only take the first 65 items from both (as you said correspondence for the first 65)\n",
    "merged = []\n",
    "for i in range(65):\n",
    "    # Handle if your JSON structure is a dict with a 'predict' or 'label' key\n",
    "    pred = predicts[i]['predict'] if isinstance(predicts[i], dict) and 'predict' in predicts[i] else predicts[i]\n",
    "    lab = labels[i]['label'] if isinstance(labels[i], dict) and 'label' in labels[i] else labels[i]\n",
    "    # Parse label if it's a JSON string\n",
    "    if isinstance(lab, str):\n",
    "        try:\n",
    "            lab = json.loads(lab)\n",
    "        except Exception:\n",
    "            pass  # Keep as string if not parsable\n",
    "    merged.append({\n",
    "        'predict': pred,\n",
    "        'label': lab\n",
    "    })\n",
    "\n",
    "# Save to output\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(merged, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved merged data to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a286b6-6e47-4f3e-888f-d609089fcee2",
   "metadata": {},
   "source": [
    "## Evaluation of complete_predict_raw_llama31_8b.json , complete_predict_4omini_structed.json , complete_predict_4_1mini_structed.json and our fine-tuned model results: generated_predictions_all_llama31_2.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac24aff7-bd53-4387-9a4d-360e1ef7c2c7",
   "metadata": {},
   "source": [
    "#### Evaluation of raw llama-3.1-8b (we got the results using MonsterAPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "add9cce3-990d-4e5f-813d-5a18c6745a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy : 0.4308\n",
      "Precision: 0.5714\n",
      "Recall   : 0.1053\n",
      "F1-score : 0.1778\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "## this pattern for json:\n",
    "a = []\n",
    "with open(\"testing/complete_predict_4_1mini_structed.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    a = json.load(f)\n",
    "\n",
    "#this pattern for jsonl:\n",
    "# a = []\n",
    "# with open(\"testing/generated_predictions_all_llama31_2.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for i, line in enumerate(f):\n",
    "#         if i >= 65:\n",
    "#             break\n",
    "#         a.append(json.loads(line))\n",
    "        \n",
    "def eval_metrics(vul_range):\n",
    "    \"\"\"\n",
    "    Accepts strings like '7', '7-12', '7,8,9', '7,9-12'\n",
    "    Returns a set of all lines covered.\n",
    "    \"\"\"\n",
    "    if not vul_range or vul_range.strip() == \"\":\n",
    "        return set()\n",
    "    vul_range = vul_range.replace(\" \", \"\")  # Remove whitespace\n",
    "    result = set()\n",
    "    for part in vul_range.split(\",\"):\n",
    "        if \"-\" in part:\n",
    "            try:\n",
    "                start, end = map(int, part.split(\"-\"))\n",
    "                result.update(range(start, end + 1))\n",
    "            except ValueError:\n",
    "                continue\n",
    "        else:\n",
    "            try:\n",
    "                result.add(int(part))\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return result\n",
    "\n",
    "def parse_field(field):\n",
    "    if isinstance(field, dict):\n",
    "        return field\n",
    "    if not field or field.strip() == \"\":\n",
    "        return {\"vulnerableLines\": \"\"}\n",
    "    try:\n",
    "        return json.loads(field)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"vulnerableLines\": \"\"}\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "results = []\n",
    "\n",
    "for i, sample in enumerate(a):\n",
    "    label_dict = parse_field(sample.get(\"label\", \"\"))\n",
    "    pred_dict = parse_field(sample.get(\"predict\", \"\"))\n",
    "\n",
    "    label_range_str = label_dict.get(\"vulnerableLines\", \"\")\n",
    "    pred_range_str = pred_dict.get(\"vulnerableLines\", \"\")\n",
    "\n",
    "    label_lines = eval_metrics(label_range_str)\n",
    "    pred_lines = eval_metrics(pred_range_str)\n",
    "    # print(label_lines)\n",
    "    # print(pred_lines)\n",
    "    # print(label_lines & pred_lines)\n",
    "    if not label_lines and not pred_lines:\n",
    "        y_true.append(0)\n",
    "        y_pred.append(0)\n",
    "        results.append({\"id\": i, \"result\": \"TN\"})\n",
    "    elif label_lines and not pred_lines:\n",
    "        y_true.append(1)\n",
    "        y_pred.append(0)\n",
    "        results.append({\"id\": i, \"result\": \"FN\"})\n",
    "    elif not label_lines and pred_lines:\n",
    "        y_true.append(0)\n",
    "        y_pred.append(1)\n",
    "        results.append({\"id\": i, \"result\": \"FP\"})\n",
    "    else:\n",
    "        overlap = label_lines & pred_lines\n",
    "        if overlap:\n",
    "            y_true.append(1)\n",
    "            y_pred.append(1)\n",
    "            results.append({\"id\": i, \"result\": \"TP\"})\n",
    "        else:\n",
    "            y_true.append(1)\n",
    "            y_pred.append(0)\n",
    "            results.append({\"id\": i, \"result\": \"FN (no overlap)\"})\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "# Optional: Save results\n",
    "with open(\"eval_complete_predict_4_1mini_structed.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in results:\n",
    "        f.write(json.dumps(r) + \"\\n\")\n",
    "        #print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67028b6c-f700-46d4-952e-93efc2f1e951",
   "metadata": {},
   "source": [
    "## Factual consistency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7cb445bf-dc98-4a76-ae84-4892bff53fe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4dcb9d76faa45f2a3a7d05baaefa672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d44572d4b3f4b96934c54a20c9aa450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3813716f7004fcbbab670f6ac2524ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b3de55b3964e51a1b0f81480598816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f697db1d004e2cb667355091ec7546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8a1983aee6417abfbc7ae9018be08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809eadbb0685465f81f5e8a952c33445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532278e583d04ea7a80437b3b8cb04e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad09cd50a2cf45bb9d8f277cadc8729f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b4853b31ad44638000e7dacdd9b740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Factual Consistency Score (Label Predict Reason): 0.5835\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "tqdm(disable=True) \n",
    "\n",
    "# Load lightweight sentence embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Safe JSON parser\n",
    "def safe_parse_json(field):\n",
    "    if isinstance(field, dict):\n",
    "        return field\n",
    "    if not field or (isinstance(field, str) and field.strip() == \"\"):\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(field)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "# Compute semantic similarity between label and predicted vulnerabilityReason\n",
    "factual_scores = []\n",
    "\n",
    "for sample in a:  # Use all samples\n",
    "    label = safe_parse_json(sample.get(\"label\", \"\"))\n",
    "    pred = safe_parse_json(sample.get(\"predict\", \"\"))\n",
    "\n",
    "    label_reason = label.get(\"vulnerabilityReason\", \"\")\n",
    "    pred_reason = pred.get(\"vulnerabilityReason\", \"\")\n",
    "\n",
    "    if not label_reason.strip() or not pred_reason.strip():\n",
    "        continue\n",
    "\n",
    "    label_emb = model.encode(label_reason, convert_to_tensor=True)\n",
    "    pred_emb = model.encode(pred_reason, convert_to_tensor=True)\n",
    "\n",
    "    score = util.cos_sim(label_emb, pred_emb).item()\n",
    "    factual_scores.append(score)\n",
    "\n",
    "# Report\n",
    "avg_score = sum(factual_scores) / len(factual_scores) if factual_scores else 0\n",
    "print(f\"Average Factual Consistency Score (Label Predict Reason): {avg_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd639c1c-f7ec-4cef-ac43-049e68103a95",
   "metadata": {},
   "source": [
    "## Code-Aware Similarity (VulnerableCode <--> FixedCode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "feb6e4d4-4209-41c1-b09c-283d4c562b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cb6fe0307c4fd29c3be24e476d1763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6e8188e11f400883ea4a0ad82ab8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e891815809f8469c9ead9760f684252d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9aa48150bd34b98b69d794e0a71939e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e8af1860524259a63869dd01268d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883f5810a195483da99a7892c2737e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0dbdb06efef496fb78beb0208ce2d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01fa4c492eb4923b33f52c10900b03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3d3c0b0b014185ac8dfc32aa55016e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551e97294a694d3a8ff104ed99873db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Code-Aware Similarity (fixed code of prediction ↔ label): 0.6601\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "\n",
    "# Load lightweight sentence embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Safe JSON parser\n",
    "def safe_parse_json(field):\n",
    "    if not field or (isinstance(field, str) and field.strip() == \"\"):\n",
    "        return {}\n",
    "    if isinstance(field, dict):\n",
    "        return field\n",
    "    try:\n",
    "        return json.loads(field)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "# Helper to extract code snippet from possible dict/list structure\n",
    "def extract_code_snippet(item):\n",
    "    if isinstance(item, str):\n",
    "        return item\n",
    "    elif isinstance(item, dict):\n",
    "        # Change 'code' to another key if needed\n",
    "        return item.get(\"code\", \"\") or str(item)\n",
    "    return str(item)\n",
    "\n",
    "code_similarity_scores = []\n",
    "\n",
    "for sample in a:  # Use all samples\n",
    "    pred = safe_parse_json(sample.get(\"predict\", \"\"))\n",
    "\n",
    "    vuln_code = pred.get(\"fixedCode\", [])\n",
    "    if isinstance(vuln_code, list):\n",
    "        vuln_code = \"\\n\".join([extract_code_snippet(item) for item in vuln_code])\n",
    "    elif vuln_code is None:\n",
    "        vuln_code = \"\"\n",
    "    else:\n",
    "        vuln_code = extract_code_snippet(vuln_code)\n",
    "    \n",
    "    pred = safe_parse_json(sample.get(\"label\", \"\"))\n",
    "    fixed_code = pred.get(\"fixedCode\", [])\n",
    "    if isinstance(fixed_code, list):\n",
    "        fixed_code = \"\\n\".join([extract_code_snippet(item) for item in fixed_code])\n",
    "    elif fixed_code is None:\n",
    "        fixed_code = \"\"\n",
    "    else:\n",
    "        fixed_code = extract_code_snippet(fixed_code)\n",
    "\n",
    "    # Both should be strings at this point\n",
    "    if not vuln_code.strip() or not fixed_code.strip():\n",
    "        continue\n",
    "\n",
    "    vuln_emb = model.encode(vuln_code, convert_to_tensor=True)\n",
    "    fixed_emb = model.encode(fixed_code, convert_to_tensor=True)\n",
    "\n",
    "    score = util.cos_sim(vuln_emb, fixed_emb).item()\n",
    "    code_similarity_scores.append(score)\n",
    "\n",
    "# Report\n",
    "avg_score = sum(code_similarity_scores) / len(code_similarity_scores) if code_similarity_scores else 0\n",
    "print(f\"Average Code-Aware Similarity (fixed code of prediction ↔ label): {avg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c06318b-0c7e-41cc-9e47-514a62236b3f",
   "metadata": {},
   "source": [
    "# Risk Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3a378765-daef-4bb3-84db-dcc2ca4debce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee43784c1b6443298234658ca3f03200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325a25d76f0142b9a0d28504d20e9c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957dd44caf6d4a04be8464aaad40a3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b3113d82ce46dab8527da72d43bcc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c3788241a4427fbe22b36b6292f0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f0994742974c29828ae4994a3533d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216fcb05eac8428b8e6e45b89f1fc37b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246dec39031c4eb8810dfc10eeda4a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fec17180bab4990adeaae3eff3bf957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce070ac682041bf84d57893b34845ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Risk Assessment Alignment (Risk ↔ Risk): 0.5814\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "\n",
    "# Load lightweight sentence embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Safe JSON parser\n",
    "# def safe_parse_json(field):\n",
    "#     if not field or field.strip() == \"\":\n",
    "#         return {}\n",
    "#     if isinstance(field, dict):\n",
    "#         return field\n",
    "#     try:\n",
    "#         return json.loads(field)\n",
    "#     except json.JSONDecodeError:\n",
    "#         return {}\n",
    "def safe_parse_json(field):\n",
    "    if isinstance(field, dict):\n",
    "        return field\n",
    "    if not field or (isinstance(field, str) and field.strip() == \"\"):\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(field)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "# Compute semantic similarity between vulnerabilityReason and potentialSecurityRisk\n",
    "risk_alignment_scores = []\n",
    "\n",
    "for sample in a:  # Use all samples\n",
    "    pred = safe_parse_json(sample.get(\"predict\", \"\"))\n",
    "    \n",
    "    reason = pred.get(\"potentialSecurityRisk\", \"\")\n",
    "    \n",
    "    pred = safe_parse_json(sample.get(\"label\", \"\"))\n",
    "    risk = pred.get(\"potentialSecurityRisk\", \"\")\n",
    "\n",
    "    if not reason.strip() or not risk.strip():\n",
    "        continue\n",
    "\n",
    "    reason_emb = model.encode(reason, convert_to_tensor=True)\n",
    "    risk_emb = model.encode(risk, convert_to_tensor=True)\n",
    "\n",
    "    score = util.cos_sim(reason_emb, risk_emb).item()\n",
    "    risk_alignment_scores.append(score)\n",
    "\n",
    "# Report\n",
    "avg_score = sum(risk_alignment_scores) / len(risk_alignment_scores) if risk_alignment_scores else 0\n",
    "print(f\"Average Risk Assessment Alignment (Risk ↔ Risk): {avg_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad93f8aa-5f68-4551-a790-912772a7d39d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
