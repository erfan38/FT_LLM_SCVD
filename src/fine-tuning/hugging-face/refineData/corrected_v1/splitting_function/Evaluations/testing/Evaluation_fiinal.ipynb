{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcf793b2-6424-4a57-b408-9c695a28ac1a",
   "metadata": {},
   "source": [
    "## Evaluation of\n",
    "## \"complete_predict_raw_llama31_8b.json\" ,\n",
    "## \"complete_predict_4omini_structed.json\" ,\n",
    "## \"complete_predict_4_1mini_structed.json\" and\n",
    "## our fine-tuned model results: \"generated_predictions_all_llama31_2.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a15b92b-9319-43ef-ac2c-6caac8fc1011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy : 0.8923\n",
      "Precision: 0.9429\n",
      "Recall   : 0.8684\n",
      "F1-score : 0.9041\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "a = []\n",
    "input_file = \"generated_predictions_all_llama31_2.jsonl\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                 \n",
    "#     a = json.load(f)    ##########     # Uncomment this for .json files (not .jsonl):\n",
    "\n",
    "\n",
    "##########################  # Uncomment this for JSONL :)\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 65:\n",
    "            break\n",
    "        a.append(json.loads(line))\n",
    "        \n",
    "def eval_metrics(vul_range):\n",
    "    \"\"\"\n",
    "    Accepts strings like '7', '7-12', '7,8,9', '7,9-12'\n",
    "    Returns a set of all lines covered.\n",
    "    \"\"\"\n",
    "    if not vul_range or vul_range.strip() == \"\":\n",
    "        return set()\n",
    "    vul_range = vul_range.replace(\" \", \"\")  # Remove whitespace\n",
    "    result = set()\n",
    "    for part in vul_range.split(\",\"):\n",
    "        if \"-\" in part:\n",
    "            try:\n",
    "                start, end = map(int, part.split(\"-\"))\n",
    "                result.update(range(start, end + 1))\n",
    "            except ValueError:\n",
    "                continue\n",
    "        else:\n",
    "            try:\n",
    "                result.add(int(part))\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return result\n",
    "\n",
    "def parse_field(field):\n",
    "    if isinstance(field, dict):\n",
    "        return field\n",
    "    if not field or field.strip() == \"\":\n",
    "        return {\"vulnerableLines\": \"\"}\n",
    "    try:\n",
    "        return json.loads(field)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"vulnerableLines\": \"\"}\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "results = []\n",
    "\n",
    "for i, sample in enumerate(a):\n",
    "    label_dict = parse_field(sample.get(\"label\", \"\"))\n",
    "    pred_dict = parse_field(sample.get(\"predict\", \"\"))\n",
    "\n",
    "    label_range_str = label_dict.get(\"vulnerableLines\", \"\")\n",
    "    pred_range_str = pred_dict.get(\"vulnerableLines\", \"\")\n",
    "\n",
    "    label_lines = eval_metrics(label_range_str)\n",
    "    pred_lines = eval_metrics(pred_range_str)\n",
    "    # print(label_lines)\n",
    "    # print(pred_lines)\n",
    "    # print(label_lines & pred_lines)\n",
    "    if not label_lines and not pred_lines:\n",
    "        y_true.append(0)\n",
    "        y_pred.append(0)\n",
    "        results.append({\"id\": i, \"result\": \"TN\"})\n",
    "    elif label_lines and not pred_lines:\n",
    "        y_true.append(1)\n",
    "        y_pred.append(0)\n",
    "        results.append({\"id\": i, \"result\": \"FN\"})\n",
    "    elif not label_lines and pred_lines:\n",
    "        y_true.append(0)\n",
    "        y_pred.append(1)\n",
    "        results.append({\"id\": i, \"result\": \"FP\"})\n",
    "    else:\n",
    "        overlap = label_lines & pred_lines\n",
    "        if overlap:\n",
    "            y_true.append(1)\n",
    "            y_pred.append(1)\n",
    "            results.append({\"id\": i, \"result\": \"TP\"})\n",
    "        else:\n",
    "            y_true.append(1)\n",
    "            y_pred.append(0)\n",
    "            results.append({\"id\": i, \"result\": \"FN (no overlap)\"})\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "# Optional: Save results\n",
    "with open(f\"eval_{input_file}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in results:\n",
    "        f.write(json.dumps(r) + \"\\n\")\n",
    "        #print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7a178-8ba4-415a-beb5-161d0b09e153",
   "metadata": {},
   "source": [
    "## Factual Consistency: predict[\"vulnerabilityReason\"] <--> label[\"vulnerabilityReason\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4870b73-e1d1-467f-ad99-7dcdb902504d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Factual Consistency Score (Label Predict Reason): 0.7737\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "tqdm(disable=True) \n",
    "\n",
    "# Load lightweight sentence embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Safe JSON parser\n",
    "def safe_parse_json(field):\n",
    "    if isinstance(field, dict):\n",
    "        return field\n",
    "    if not field or (isinstance(field, str) and field.strip() == \"\"):\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(field)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "# Compute semantic similarity between label and predicted vulnerabilityReason\n",
    "factual_scores = []\n",
    "\n",
    "for sample in a:  # Use all samples\n",
    "    label = safe_parse_json(sample.get(\"label\", \"\"))\n",
    "    pred = safe_parse_json(sample.get(\"predict\", \"\"))\n",
    "\n",
    "    label_reason = label.get(\"vulnerabilityReason\", \"\")\n",
    "    pred_reason = pred.get(\"vulnerabilityReason\", \"\")\n",
    "\n",
    "    if not label_reason.strip() or not pred_reason.strip():\n",
    "        continue\n",
    "\n",
    "    label_emb = model.encode(label_reason, convert_to_tensor=True)\n",
    "    pred_emb = model.encode(pred_reason, convert_to_tensor=True)\n",
    "\n",
    "    score = util.cos_sim(label_emb, pred_emb).item()\n",
    "    factual_scores.append(score)\n",
    "\n",
    "# Report\n",
    "avg_score = sum(factual_scores) / len(factual_scores) if factual_scores else 0\n",
    "print(f\"Average Factual Consistency Score (Label Predict Reason): {avg_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3109e35-c814-4d20-a1c8-d3eb9d9c1100",
   "metadata": {},
   "source": [
    "## (FixedCode <--> FixedCode)\n",
    "## (vulnerableCode <--> vulnerableCode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d75bbe82-2323-40d1-80d0-d2cbce4ca7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Code-Aware Similarity (fixed code of prediction ↔ label): 0.8862\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "\n",
    "# Load lightweight sentence embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Safe JSON parser\n",
    "def safe_parse_json(field):\n",
    "    if not field or (isinstance(field, str) and field.strip() == \"\"):\n",
    "        return {}\n",
    "    if isinstance(field, dict):\n",
    "        return field\n",
    "    try:\n",
    "        return json.loads(field)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "# Helper to extract code snippet from possible dict/list structure\n",
    "def extract_code_snippet(item):\n",
    "    if isinstance(item, str):\n",
    "        return item\n",
    "    elif isinstance(item, dict):\n",
    "        # Change 'code' to another key if needed\n",
    "        return item.get(\"code\", \"\") or str(item)\n",
    "    return str(item)\n",
    "\n",
    "code_similarity_scores = []\n",
    "\n",
    "for sample in a:  # Use all samples\n",
    "    pred = safe_parse_json(sample.get(\"predict\", \"\"))\n",
    "\n",
    "    vuln_code = pred.get(\"fixedCode\", [])\n",
    "    if isinstance(vuln_code, list):\n",
    "        vuln_code = \"\\n\".join([extract_code_snippet(item) for item in vuln_code])\n",
    "    elif vuln_code is None:\n",
    "        vuln_code = \"\"\n",
    "    else:\n",
    "        vuln_code = extract_code_snippet(vuln_code)\n",
    "    \n",
    "    pred = safe_parse_json(sample.get(\"label\", \"\"))\n",
    "    fixed_code = pred.get(\"fixedCode\", [])\n",
    "    if isinstance(fixed_code, list):\n",
    "        fixed_code = \"\\n\".join([extract_code_snippet(item) for item in fixed_code])\n",
    "    elif fixed_code is None:\n",
    "        fixed_code = \"\"\n",
    "    else:\n",
    "        fixed_code = extract_code_snippet(fixed_code)\n",
    "\n",
    "    # Both should be strings at this point\n",
    "    if not vuln_code.strip() or not fixed_code.strip():\n",
    "        continue\n",
    "\n",
    "    vuln_emb = model.encode(vuln_code, convert_to_tensor=True)\n",
    "    fixed_emb = model.encode(fixed_code, convert_to_tensor=True)\n",
    "\n",
    "    score = util.cos_sim(vuln_emb, fixed_emb).item()\n",
    "    code_similarity_scores.append(score)\n",
    "\n",
    "# Report\n",
    "avg_score = sum(code_similarity_scores) / len(code_similarity_scores) if code_similarity_scores else 0\n",
    "print(f\"Average Code-Aware Similarity (fixed code of prediction ↔ label): {avg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c109bf0-e898-471b-bc57-e56da936c01b",
   "metadata": {},
   "source": [
    "## (potentialSecurityRisk <--> potentialSecurityRisk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5bd1e27-89c4-4066-9950-f7e14a2336c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Risk Assessment Alignment (Risk ↔ Risk): 0.6891\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "\n",
    "# Load lightweight sentence embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Safe JSON parser\n",
    "# def safe_parse_json(field):\n",
    "#     if not field or field.strip() == \"\":\n",
    "#         return {}\n",
    "#     if isinstance(field, dict):\n",
    "#         return field\n",
    "#     try:\n",
    "#         return json.loads(field)\n",
    "#     except json.JSONDecodeError:\n",
    "#         return {}\n",
    "def safe_parse_json(field):\n",
    "    if isinstance(field, dict):\n",
    "        return field\n",
    "    if not field or (isinstance(field, str) and field.strip() == \"\"):\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(field)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "# Compute semantic similarity between vulnerabilityReason and potentialSecurityRisk\n",
    "risk_alignment_scores = []\n",
    "\n",
    "for sample in a:  # Use all samples\n",
    "    pred = safe_parse_json(sample.get(\"predict\", \"\"))\n",
    "    \n",
    "    reason = pred.get(\"potentialSecurityRisk\", \"\")\n",
    "    \n",
    "    pred = safe_parse_json(sample.get(\"label\", \"\"))\n",
    "    risk = pred.get(\"potentialSecurityRisk\", \"\")\n",
    "\n",
    "    if not reason.strip() or not risk.strip():\n",
    "        continue\n",
    "\n",
    "    reason_emb = model.encode(reason, convert_to_tensor=True)\n",
    "    risk_emb = model.encode(risk, convert_to_tensor=True)\n",
    "\n",
    "    score = util.cos_sim(reason_emb, risk_emb).item()\n",
    "    risk_alignment_scores.append(score)\n",
    "\n",
    "# Report\n",
    "avg_score = sum(risk_alignment_scores) / len(risk_alignment_scores) if risk_alignment_scores else 0\n",
    "print(f\"Average Risk Assessment Alignment (Risk ↔ Risk): {avg_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6420fd77-17fb-43ae-8ff9-f5a06edd93b5",
   "metadata": {},
   "source": [
    "## OpenAI's embedding Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "806763de-0fc0-4909-9eca-306200adcdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai, json\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e927e3bd-9a68-4d2f-8163-a4ad2aa26db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23ee9a54-7c21-43c3-bc73-1cf0c36fbe68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Semantic Similarity:0.7413\n"
     ]
    }
   ],
   "source": [
    "file_path = \"complete_predict_4omini_structed.json\"  # complete_predict_4omini_structed.json  #generated_predictions_all_llama31_2.jsonl ## complete_predict_4_1mini_structed.jso\n",
    "max_samples = 65\n",
    "\n",
    "data = load_json_or_jsonl(file_path, max_items=max_samples)\n",
    "\n",
    "all_reasons = []\n",
    "index_map = []\n",
    "\n",
    "def load_json_or_jsonl(file_path, max_items=None):\n",
    "    \"\"\"\n",
    "    Loads a file as either .json (object/array) or .jsonl (one JSON object per line).\n",
    "    Returns a list of dicts.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            # If it's a list, you can optionally slice for max_items\n",
    "            if isinstance(data, list):\n",
    "                if max_items is not None:\n",
    "                    data = data[:max_items]\n",
    "                return data\n",
    "            else:\n",
    "                # If it's a dict, wrap in a list for compatibility\n",
    "                return [data]\n",
    "        except json.JSONDecodeError:\n",
    "            # If not a valid JSON array/object, treat as JSONL\n",
    "            items = []\n",
    "            for i, line in enumerate(f):\n",
    "                if max_items is not None and i >= max_items:\n",
    "                    break\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                items.append(json.loads(line))\n",
    "            return items\n",
    "\n",
    "def safe_parse_json(field):\n",
    "    if isinstance(field, dict):\n",
    "        return field\n",
    "    if not field or (isinstance(field, str) and field.strip() == \"\"):\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(field)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "def get_code_as_str(val):\n",
    "    if isinstance(val, list):\n",
    "        return \"\\n\".join(str(x) for x in val).strip()\n",
    "    elif isinstance(val, str):\n",
    "        return val.strip()\n",
    "    elif val is None:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return str(val).strip()\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    if not text.strip():\n",
    "        return None\n",
    "    response = openai.embeddings.create(input=[text], model=model)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# ----- Main Code -----\n",
    "\n",
    "for i, entry in enumerate(data):\n",
    "    pred = safe_parse_json(entry.get(\"predict\", \"\"))\n",
    "    label = safe_parse_json(entry.get(\"label\", \"\"))\n",
    "\n",
    "    pred_reason = get_code_as_str(pred.get(\"fixedCode\", \"\"))\n",
    "    label_reason = get_code_as_str(label.get(\"fixedCode\", \"\"))\n",
    "\n",
    "    if pred_reason and label_reason:\n",
    "        all_reasons.extend([pred_reason, label_reason])\n",
    "        index_map.append(i)\n",
    "\n",
    "# Get all embeddings in one batch call\n",
    "response = openai.embeddings.create(\n",
    "    input=all_reasons,\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "embeddings = [item.embedding for item in response.data]\n",
    "\n",
    "# Compute similarity\n",
    "similarity_scores = []\n",
    "for i, idx in enumerate(index_map):\n",
    "    emb_pred = embeddings[2 * i]\n",
    "    emb_label = embeddings[2 * i + 1]\n",
    "    score = cosine_similarity([emb_pred], [emb_label])[0][0]\n",
    "\n",
    "    pred = safe_parse_json(data[idx].get(\"predict\", \"\"))\n",
    "    label = safe_parse_json(data[idx].get(\"label\", \"\"))\n",
    "\n",
    "    similarity_scores.append({\n",
    "        \"predict\": pred.get(\"fixedCode\", \"\"),\n",
    "        \"label\": label.get(\"fixedCode\", \"\"),\n",
    "        \"similarity\": score\n",
    "    })\n",
    "sum_similarity = 0\n",
    "for item in similarity_scores[:]:\n",
    "    sum_similarity = sum_similarity + item[\"similarity\"]\n",
    "avg_score = sum_similarity / len(similarity_scores)\n",
    "print(f\"Average Semantic Similarity:{avg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fac0f5-e3da-418d-babd-0540a8537a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5957ee-3d16-488e-a8e5-7af5432d48b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4103ffa-245e-47e7-858b-b7bd437a27fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
