{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b352cbd5-4bed-4823-83eb-226ca90b7ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = \"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\"\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "input_file = \"testing/prompt.json\"\n",
    "output_file = \"testing/predict_4_1mini_structed.json\"\n",
    "\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict\n",
    "\n",
    "def get_structured_predictions(prompts: List[str], model_name: str) -> List[Dict]:\n",
    "\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    tool_schema = {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"report_vulnerabilities\",\n",
    "            \"description\": \"Extract vulnerabilities from a smart contract and return structured details.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"vulnerableLines\": {\"type\": \"string\"},\n",
    "                    \"vulnerableCode\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"}\n",
    "                    },\n",
    "                    \"vulnerabilityReason\": {\"type\": \"string\"},\n",
    "                    \"potentialSecurityRisk\": {\"type\": \"string\"},\n",
    "                    \"fixedCode\": {\"type\": \"string\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    instruction = (\n",
    "        \"You are a careful cyber-security expert who analyzes smart contracts. \"\n",
    "        \"Detect all vulnerabilities in the code and provide your answer using the tool function:\\n\"\n",
    "        \"Smart contract:\\n\"\n",
    "    )\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for i, code in enumerate(prompts):\n",
    "        full_prompt = instruction + code\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "                tools=[tool_schema],\n",
    "                tool_choice=\"auto\",\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            tool_call = response.choices[0].message.tool_calls[0]\n",
    "            arguments = json.loads(tool_call.function.arguments)\n",
    "            predictions.append({\"predict\": arguments})\n",
    "            print(f\"Prompt {i+1} processed.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Prompt {i+1} failed:\", e)\n",
    "            predictions.append({\"predict\": {}})\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "25e1be37-9064-488c-8901-5c5056f892c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1 processed.\n",
      "Prompt 2 processed.\n",
      "Prompt 3 processed.\n",
      "Prompt 4 processed.\n",
      "Prompt 5 processed.\n",
      "Prompt 6 processed.\n",
      "Prompt 7 processed.\n",
      "Prompt 8 processed.\n",
      "Prompt 9 processed.\n",
      "Prompt 10 processed.\n",
      "Prompt 11 processed.\n",
      "Prompt 12 processed.\n",
      "Prompt 13 processed.\n",
      "Prompt 14 processed.\n",
      "Prompt 15 processed.\n",
      "Prompt 16 processed.\n",
      "Prompt 17 processed.\n",
      "Prompt 18 processed.\n",
      "Prompt 19 processed.\n",
      "Prompt 20 processed.\n",
      "Prompt 21 processed.\n",
      "Prompt 22 processed.\n",
      "Prompt 23 processed.\n",
      "Prompt 24 processed.\n",
      "Prompt 25 processed.\n",
      "Prompt 26 processed.\n",
      "Prompt 27 processed.\n",
      "Prompt 28 processed.\n",
      "Prompt 29 processed.\n",
      "Prompt 30 processed.\n",
      "Prompt 31 processed.\n",
      "Prompt 32 processed.\n",
      "Prompt 33 processed.\n",
      "Prompt 34 processed.\n",
      "Prompt 35 processed.\n",
      "Prompt 36 processed.\n",
      "Prompt 37 processed.\n",
      "Prompt 38 processed.\n",
      "Prompt 39 processed.\n",
      "Prompt 40 processed.\n",
      "Prompt 41 processed.\n",
      "Prompt 42 processed.\n",
      "Prompt 43 processed.\n",
      "Prompt 44 processed.\n",
      "Prompt 45 processed.\n",
      "Prompt 46 processed.\n",
      "Prompt 47 processed.\n",
      "Prompt 48 processed.\n",
      "Prompt 49 processed.\n",
      "Prompt 50 processed.\n",
      "Done. 50 predictions saved.\n"
     ]
    }
   ],
   "source": [
    "MAX_SAMPLE = 50\n",
    "\n",
    "with open(\"testing/prompt.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    prompts = [item[\"prompt\"] for item in data]\n",
    "\n",
    "predictions = get_structured_predictions(prompts[:MAX_SAMPLE], \n",
    "                                          # model_name=\"ft:gpt-4o-mini-2024-07-18:personal:smart-4o-mini:Bd4faLMY\"\n",
    "                                         model_name=\"gpt-4o-mini\"\n",
    "                                        )\n",
    "\n",
    "# Save to file\n",
    "with open(\"testing/predict_4omini_structed_mohammad.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(predictions, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Done. {len(predictions)} predictions saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1ebbb213-6f72-4010-95f9-482b65a66d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "def eval_metrics(vul_range):\n",
    "    \"\"\"Convert line range string (like '3-5,7') to a set of line numbers.\"\"\"\n",
    "    if not vul_range or vul_range.strip() == \"\":\n",
    "        return set()\n",
    "    result = set()\n",
    "    for part in vul_range.replace(\" \", \"\").split(\",\"):\n",
    "        if \"-\" in part:\n",
    "            try:\n",
    "                start, end = map(int, part.split(\"-\"))\n",
    "                result.update(range(start, end + 1))\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            try:\n",
    "                result.add(int(part))\n",
    "            except:\n",
    "                continue\n",
    "    return result\n",
    "\n",
    "def parse_json_string(s):\n",
    "    \"\"\"Safely parse a JSON string. Return dict with at least 'vulnerableLines' key.\"\"\"\n",
    "    if isinstance(s, dict):\n",
    "        return s\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except:\n",
    "        return {\"vulnerableLines\": \"\"}\n",
    "\n",
    "def evaluate_preds_and_labels(preds, labels):\n",
    "    \"\"\"\n",
    "    preds: list of dicts with key 'predict' (json string)\n",
    "    labels: list of dicts with key 'label' (json string)\n",
    "    Returns: metrics + list of TP/TN/FP/FN per sample\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    results = []\n",
    "\n",
    "    for i, (p, l) in enumerate(zip(preds, labels)):\n",
    "        pred_dict = parse_json_string(p.get(\"predict\", \"\"))\n",
    "        label_dict = parse_json_string(l.get(\"label\", \"\"))\n",
    "\n",
    "        pred_lines = eval_metrics(pred_dict.get(\"vulnerableLines\", \"\"))\n",
    "        label_lines = eval_metrics(label_dict.get(\"vulnerableLines\", \"\"))\n",
    "        # print(f\"label_line: {label_lines}\")\n",
    "        # print(f\"pred_line: {pred_lines}\")\n",
    "        # print(label_lines & pred_lines)\n",
    "        # print(100*\"*\")\n",
    "        if not label_lines and not pred_lines:\n",
    "            y_true.append(0)\n",
    "            y_pred.append(0)\n",
    "            results.append({\"id\": i, \"result\": \"TN\"})\n",
    "        elif label_lines and not pred_lines:\n",
    "            y_true.append(1)\n",
    "            y_pred.append(0)\n",
    "            results.append({\"id\": i, \"result\": \"FN\"})\n",
    "        elif not label_lines and pred_lines:\n",
    "            y_true.append(0)\n",
    "            y_pred.append(1)\n",
    "            results.append({\"id\": i, \"result\": \"FP\"})\n",
    "        else:\n",
    "            if label_lines & pred_lines:\n",
    "                y_true.append(1)\n",
    "                y_pred.append(1)\n",
    "                results.append({\"id\": i, \"result\": \"TP\"})\n",
    "            else:\n",
    "                y_true.append(1)\n",
    "                y_pred.append(0)\n",
    "                results.append({\"id\": i, \"result\": \"FN (no overlap)\"})\n",
    "    print(\"y_true   : \", y_true)\n",
    "    print(\"y_predict: \", y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"results\": results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9d710c81-dcbf-41b2-a432-ed2b0796e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util as sbert_util\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "\n",
    "# Load models once\n",
    "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # for text fields\n",
    "code_model_name = \"microsoft/unixcoder-base\"\n",
    "code_tokenizer = AutoTokenizer.from_pretrained(code_model_name)\n",
    "code_model = AutoModel.from_pretrained(code_model_name)\n",
    "\n",
    "def parse_json_string(s):\n",
    "    \"\"\"Safely parse a JSON string to a dict.\"\"\"\n",
    "    if isinstance(s, dict):\n",
    "        return s\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def get_code_embedding(code_text):\n",
    "    \"\"\"Generate normalized embedding for code using UnixCoder.\"\"\"\n",
    "    tokens = code_tokenizer(code_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        output = code_model(**tokens)\n",
    "    pooled = output.last_hidden_state.mean(dim=1)\n",
    "    return F.normalize(pooled, dim=1)\n",
    "\n",
    "def calculate_field_consistency(preds, labels, fields):\n",
    "    \"\"\"\n",
    "    Computes average cosine similarity for each field using appropriate embedding models.\n",
    "\n",
    "    Args:\n",
    "        preds (List[Dict]): List of predictions with 'predict' (dict or string).\n",
    "        labels (List[Dict]): List of labels with 'label' (stringified JSON).\n",
    "        fields (List[str]): Fields to check for semantic similarity.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Average cosine similarity per field.\n",
    "    \"\"\"\n",
    "    field_scores = {field: [] for field in fields}\n",
    "\n",
    "    for pred_item, label_item in zip(preds, labels):\n",
    "        pred_dict = parse_json_string(pred_item.get(\"predict\", \"\"))\n",
    "        label_dict = parse_json_string(label_item.get(\"label\", \"\"))\n",
    "\n",
    "        for field in fields:\n",
    "            pred_raw = pred_dict.get(field, \"\")\n",
    "            label_raw = label_dict.get(field, \"\")\n",
    "            \n",
    "            pred_text = str(pred_raw).strip() if not isinstance(pred_raw, str) else pred_raw.strip()\n",
    "            label_text = str(label_raw).strip() if not isinstance(label_raw, str) else label_raw.strip()\n",
    "\n",
    "\n",
    "            if not pred_text or not label_text:\n",
    "                continue  # skip empty\n",
    "\n",
    "            try:\n",
    "                # Use code embedding for code-like fields\n",
    "                if field == \"fixedCode\":\n",
    "                    pred_emb = get_code_embedding(pred_text)\n",
    "                    label_emb = get_code_embedding(label_text)\n",
    "                    score = F.cosine_similarity(pred_emb, label_emb).item()\n",
    "                else:\n",
    "                    # Default to natural language embeddings\n",
    "                    pred_emb = sbert_model.encode(pred_text, convert_to_tensor=True)\n",
    "                    label_emb = sbert_model.encode(label_text, convert_to_tensor=True)\n",
    "                    score = sbert_util.cos_sim(pred_emb, label_emb).item()\n",
    "\n",
    "                field_scores[field].append(score)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Embedding failed for field '{field}':\", e)\n",
    "                continue\n",
    "\n",
    "    # Average the scores\n",
    "    return {\n",
    "        field: sum(scores) / len(scores) if scores else 0.0\n",
    "        for field, scores in field_scores.items()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0dae98c1-6688-42e1-9c92-b1ffe16365f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sample = MAX_SAMPLE\n",
    "\n",
    "# with open(\"testing/predict_4omini_structed_mohammad.json\", \"r\") as f:\n",
    "#     preds = json.load(f)\n",
    "\n",
    "# with open(\"testing/predict_raw_llama31_8b_mohammad.json\", \"r\") as f:\n",
    "#     preds = json.load(f)\n",
    "\n",
    "# with open(\"testing/predict_4_1mini_structed_mohammad.json\", \"r\") as f:\n",
    "#     preds = json.load(f)\n",
    "\n",
    "preds = []\n",
    "with open(\"testing/generated_predictions_all_llama31_2.jsonl\", \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= max_sample:\n",
    "            break\n",
    "        preds.append(json.loads(line))\n",
    "\n",
    "\n",
    "with open(\"testing/label.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "\n",
    "preds = preds[:max_sample]\n",
    "label = labels[:max_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "23ef6d00-a277-4b54-94ce-967c7b223979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true   :  [0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1]\n",
      "y_predict:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "📊 Evaluation for model: ft_4o-mini (n=50)\n",
      "Classification Metrics:\n",
      "  Accuracy  : 0.5800\n",
      "  Precision : 0.6170\n",
      "  Recall    : 0.9062\n",
      "  F1        : 0.7342\n",
      "\n",
      "Semantic Consistency Scores:\n",
      "  vulnerabilityReason      : 0.6614\n",
      "  potentialSecurityRisk    : 0.6757\n",
      "  fixedCode                : 0.6256\n",
      "\n",
      "✅ Results saved to eval_results_ft_4o-mini.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Config\n",
    "max_sample = MAX_SAMPLE\n",
    "model_name = \"ft_4o-mini\"  # Change this depending on the model you're evaluating\n",
    "\n",
    "# Load predictions\n",
    "preds = []\n",
    "\n",
    "\n",
    "if model_name == \"ft_4o-mini\":\n",
    "    with open(\"testing/predict_Fine-tuned_4omini_structed_mohammad.json\", \"r\") as f:\n",
    "        preds = json.load(f)\n",
    "        \n",
    "elif model_name == \"4o-mini\":\n",
    "    with open(\"testing/predict_4omini_structed_mohammad.json\", \"r\") as f:\n",
    "        preds = json.load(f)\n",
    "\n",
    "elif model_name == \"raw_llama31\":\n",
    "    with open(\"testing/predict_raw_llama31_8b_mohammad.json\", \"r\") as f:\n",
    "        preds = json.load(f)\n",
    "\n",
    "elif model_name == \"4-1-mini\":\n",
    "    with open(\"testing/predict_4_1mini_structed_mohammad.json\", \"r\") as f:\n",
    "        preds = json.load(f)\n",
    "\n",
    "elif model_name == \"ft_llama31\":\n",
    "    with open(\"testing/generated_predictions_all_llama31_2.jsonl\", \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= max_sample:\n",
    "                break\n",
    "            preds.append(json.loads(line))\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported model_name: {model_name}\")\n",
    "\n",
    "# Load labels\n",
    "with open(\"testing/label.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "# Truncate to max_sample\n",
    "preds = preds[:max_sample]\n",
    "labels = labels[:max_sample]\n",
    "\n",
    "# Run evaluations\n",
    "metrics = evaluate_preds_and_labels(preds, labels)\n",
    "fields_to_compare = [\"vulnerabilityReason\", \"potentialSecurityRisk\", \"fixedCode\"]\n",
    "consistency_scores = calculate_field_consistency(preds, labels, fields_to_compare)\n",
    "\n",
    "# Final combined output\n",
    "full_eval_output = {\n",
    "    \"model_name\": model_name,\n",
    "    \"max_sample\": max_sample,\n",
    "    \"classification_metrics\": {\n",
    "        \"accuracy\": metrics[\"accuracy\"],\n",
    "        \"precision\": metrics[\"precision\"],\n",
    "        \"recall\": metrics[\"recall\"],\n",
    "        \"f1\": metrics[\"f1\"]\n",
    "    },\n",
    "    \"consistency_scores\": consistency_scores,\n",
    "    \"sample_results\": metrics[\"results\"]\n",
    "}\n",
    "\n",
    "# Save to a model-specific JSON file\n",
    "output_file = f\"eval_results_{model_name}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(full_eval_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n📊 Evaluation for model: {model_name} (n={max_sample})\")\n",
    "print(\"Classification Metrics:\")\n",
    "for k, v in full_eval_output[\"classification_metrics\"].items():\n",
    "    print(f\"  {k.capitalize():<10}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nSemantic Consistency Scores:\")\n",
    "for field, score in full_eval_output[\"consistency_scores\"].items():\n",
    "    print(f\"  {field:<25}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a6bc09-bf6f-493e-b980-794ece7025c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229abc54-8c8b-45f9-b48b-33dc29f24b95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa84c5b-835c-40d8-8313-56385e5eaf46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c768c28-03c6-477b-9c3c-e8d7b51bdb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
